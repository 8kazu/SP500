{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNpjGgtA8cGw",
        "outputId": "1116dce7-59f1-47b0-b6c6-c24b17a3b19a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1GbPDHP8zkP"
      },
      "source": [
        "2. Time2Vec モジュールの実装\n",
        "Time2Vecは、時系列データに時間情報を加えるための方法です。以下にその実装を示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LrEw5JJW80PD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Time2Vec(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(Time2Vec, self).__init__()\n",
        "        self.k = kernel_size\n",
        "        self.w = nn.Parameter(torch.randn(kernel_size))\n",
        "        self.b = nn.Parameter(torch.randn(kernel_size))\n",
        "        self.w0 = nn.Parameter(torch.randn(1))\n",
        "        self.b0 = nn.Parameter(torch.randn(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        v1 = self.w0 * x + self.b0\n",
        "        v2 = torch.sin(self.w * x + self.b)\n",
        "        return torch.cat([v1, v2], dim=-1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLN8i8om82ad"
      },
      "source": [
        "3. Transformer Encoder with LSTM\n",
        "次に、LSTMをフィードフォーワード層として使用するTransformer Encoderを実装します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2LJysMQ883R"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayerWithLSTM(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
        "        super(TransformerEncoderLayerWithLSTM, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.lstm = nn.LSTM(d_model, dim_feedforward, batch_first=True)\n",
        "        self.linear1 = nn.Linear(dim_feedforward, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(dim_feedforward)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        src2, _ = self.lstm(src.permute(1, 0, 2))  # Ensure batch_first=True\n",
        "        src2 = self.linear1(src2.permute(1, 0, 2))\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-tRQ5G_-LVg"
      },
      "source": [
        "4. 全体のTransformerモデルの実装\n",
        "最後に、Time2VecとTransformer Encoderを組み合わせてモデル全体を実装します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWpPdWiV8_h8"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, time2vec_dim, dropout=0.1):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.time2vec = Time2Vec(time2vec_dim)\n",
        "        self.embedding = nn.Linear(time2vec_dim + 2, d_model)  # Adjusted input dimension\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayerWithLSTM(d_model, nhead, dim_feedforward, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.linear = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, src):\n",
        "        time_embed = self.time2vec(src)\n",
        "        src = torch.cat([src, time_embed], dim=-1)\n",
        "        src = self.embedding(src)\n",
        "        src = src.permute(1, 0, 2)  # Adjust for MultiheadAttention shape requirement\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src)\n",
        "        src = src.permute(1, 0, 2)  # Revert shape adjustment\n",
        "        output = self.linear(src)\n",
        "        return output\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "d_model = 64\n",
        "nhead = 8\n",
        "num_encoder_layers = 6\n",
        "dim_feedforward = 64\n",
        "time2vec_dim = 10\n",
        "dropout = 0.1\n",
        "#d_modelとdim_feedforwardを一致させないと動かないが，これは正しい設計なのか？？？\n",
        "\n",
        "# モデルの初期化\n",
        "model = TimeSeriesTransformer(d_model, nhead, num_encoder_layers, dim_feedforward, time2vec_dim, dropout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBhBDfJW9Dwx"
      },
      "source": [
        "5. モデルのトレーニング\n",
        "トレーニングループを実装します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfAWaBRjHeXO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ダミーデータの生成\n",
        "batch_size=32\n",
        "num_samples = 1000\n",
        "sequence_length = 100\n",
        "\n",
        "x = torch.randn(num_samples, sequence_length, 1)\n",
        "y = torch.randn(num_samples, sequence_length, 1)\n",
        "\n",
        "# データを時間順に訓練、検証、テストに分割\n",
        "train_size = int(0.7 * num_samples)\n",
        "val_size = int(0.15 * num_samples)\n",
        "test_size = num_samples - train_size - val_size\n",
        "\n",
        "x_train = x[:train_size]\n",
        "y_train = y[:train_size]\n",
        "x_val = x[train_size:train_size + val_size]\n",
        "y_val = y[train_size:train_size + val_size]\n",
        "x_test = x[train_size + val_size:]\n",
        "y_test = y[train_size + val_size:]\n",
        "\n",
        "# DataLoaderの作成（シャッフルなし）#時系列分析なのでshuffleしなかったけど正しいのか？\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "usOJ08yGHapN",
        "outputId": "ab36cb49-77b4-4a59-8d3c-f3d4d3678ac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/100], Train Loss: 1.0081, Val Loss: 1.0110\n",
            "Epoch [10/100], Train Loss: 1.0062, Val Loss: 1.0119\n",
            "Epoch [15/100], Train Loss: 1.0040, Val Loss: 1.0128\n",
            "Epoch [20/100], Train Loss: 1.0030, Val Loss: 1.0129\n",
            "Epoch [25/100], Train Loss: 1.0029, Val Loss: 1.0132\n",
            "Epoch [30/100], Train Loss: 1.0030, Val Loss: 1.0131\n",
            "Epoch [35/100], Train Loss: 1.0024, Val Loss: 1.0135\n",
            "Epoch [40/100], Train Loss: 1.0023, Val Loss: 1.0131\n",
            "Epoch [45/100], Train Loss: 1.0021, Val Loss: 1.0135\n",
            "Epoch [50/100], Train Loss: 1.0020, Val Loss: 1.0137\n",
            "Epoch [55/100], Train Loss: 1.0011, Val Loss: 1.0137\n",
            "Epoch [60/100], Train Loss: 1.0001, Val Loss: 1.0136\n",
            "Epoch [65/100], Train Loss: 0.9987, Val Loss: 1.0150\n",
            "Epoch [70/100], Train Loss: 0.9957, Val Loss: 1.0170\n",
            "Epoch [75/100], Train Loss: 0.9915, Val Loss: 1.0200\n",
            "Epoch [80/100], Train Loss: 0.9828, Val Loss: 1.0396\n",
            "Epoch [85/100], Train Loss: 0.9721, Val Loss: 1.0408\n",
            "Epoch [90/100], Train Loss: 0.9532, Val Loss: 1.0631\n",
            "Epoch [95/100], Train Loss: 0.9240, Val Loss: 1.0833\n",
            "Epoch [100/100], Train Loss: 0.8942, Val Loss: 1.1189\n",
            "Test Loss: 0.9869\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 損失関数とオプティマイザの設定\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# トレーニングと検証ループ\n",
        "num_epochs = 100\n",
        "best_val_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # トレーニングモード\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # 検証モード\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in val_loader:\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            val_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    # ベストモデルの保存\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# ベストモデルのロード\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# テストデータでの評価\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for x_batch, y_batch in test_loader:\n",
        "        output = model(x_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        test_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqeCpzkv9DCx"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# データの準備 (例: ダミーデータ)\n",
        "batch_size = 32\n",
        "sequence_length = 100\n",
        "x = torch.randn(batch_size, sequence_length, 1)\n",
        "y = torch.randn(batch_size, sequence_length, 1)\n",
        "\n",
        "# 損失関数とオプティマイザの設定\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# トレーニングループ\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "shared_anaconda",
      "language": "python",
      "name": "shared_anaconda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
