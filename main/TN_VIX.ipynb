{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7886fouLGBra",
        "outputId": "2ff38a28-a26f-427a-8124-0117c4f198eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# GPUの使用可否を確認\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pfbvMRmbGO9t"
      },
      "outputs": [],
      "source": [
        "# 1000日のVIXデータを仮定\n",
        "np.random.seed(42)\n",
        "data = np.random.rand(1000, 1)  # VIXサンプルデータ\n",
        "\n",
        "# 時間ウィンドウのサイズを定義\n",
        "window_size = 12\n",
        "\n",
        "# 入力シーケンスxとターゲットyを生成\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(data) - window_size):\n",
        "    x.append(data[i:i + window_size])\n",
        "    y.append(data[i + window_size, 0])  # VIXが唯一の特徴量\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "# データセットを時間順に分割\n",
        "train_size = int(0.7 * len(x))\n",
        "val_size = int(0.2 * len(x))\n",
        "\n",
        "X_train, y_train = x[:train_size], y[:train_size]\n",
        "X_val, y_val = x[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
        "X_test, y_test = x[train_size + val_size:], y[train_size + val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "73dClDWxDNJ1"
      },
      "outputs": [],
      "source": [
        "# Time2Vecモジュールを定義\n",
        "class Time2Vec(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(Time2Vec, self).__init__()\n",
        "        self.k = kernel_size\n",
        "        self.w = nn.Parameter(torch.randn(kernel_size))\n",
        "        self.b = nn.Parameter(torch.randn(kernel_size))\n",
        "        self.w0 = nn.Parameter(torch.randn(1))\n",
        "        self.b0 = nn.Parameter(torch.randn(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        v1 = self.w0 * x + self.b0  # 線形部分\n",
        "        v2 = torch.sin(self.w * x + self.b)  # 正弦部分\n",
        "        return torch.cat([v1, v2], dim=-1)  # 線形部分と正弦部分を連結\n",
        "\n",
        "# Time2Vecを初期化し、データに適用\n",
        "kernel_size = 2\n",
        "time2vec = Time2Vec(kernel_size=kernel_size).to(device)\n",
        "\n",
        "def apply_time2vec(x):\n",
        "    batch_size, seq_len, num_features = x.shape\n",
        "    time_features = time2vec(torch.tensor(x, dtype=torch.float32).to(device)).detach()\n",
        "    return torch.cat([torch.tensor(x, dtype=torch.float32).to(device), time_features], dim=-1)\n",
        "\n",
        "X_train = apply_time2vec(X_train)\n",
        "X_val = apply_time2vec(X_val)\n",
        "X_test = apply_time2vec(X_test)\n",
        "\n",
        "# カスタムモデルを定義\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, lstm_hidden_size, dropout=0.1):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead)\n",
        "        self.lstm = nn.LSTM(d_model, lstm_hidden_size, batch_first=True)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(lstm_hidden_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(lstm_hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 入力プロジェクション\n",
        "        x = self.input_projection(x)\n",
        "        x = x.permute(1, 0, 2)  # 次元を調整してMultiheadAttentionの入力に合わせる\n",
        "        # MultiheadAttention\n",
        "        attn_output, _ = self.multihead_attn(x, x, x)\n",
        "        x = x + attn_output\n",
        "        x = self.layer_norm1(x)\n",
        "        x = x.permute(1, 0, 2)  # 次元を元の形式に戻す\n",
        "        # LSTM層\n",
        "        lstm_output, _ = self.lstm(x)\n",
        "        lstm_output = self.layer_norm2(lstm_output)\n",
        "        # グローバル平均プーリング\n",
        "        avg_pool_output = self.global_avg_pool(lstm_output.permute(0, 2, 1)).squeeze(-1)\n",
        "        # 全結合層とDropout\n",
        "        x = self.dropout(nn.ReLU()(self.fc1(avg_pool_output)))\n",
        "        output = self.fc2(x)\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOEs8h1qGTmR",
        "outputId": "3aa875c1-cd50-41ca-eadc-7eaa5d02d0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/100], Train Loss: 0.0887, Val Loss: 0.0892\n",
            "Epoch [20/100], Train Loss: 0.0947, Val Loss: 0.0842\n",
            "Epoch [30/100], Train Loss: 0.0928, Val Loss: 0.0847\n",
            "Epoch [40/100], Train Loss: 0.0929, Val Loss: 0.0844\n",
            "Epoch [50/100], Train Loss: 0.0903, Val Loss: 0.0841\n",
            "Epoch [60/100], Train Loss: 0.0916, Val Loss: 0.0840\n",
            "Epoch [70/100], Train Loss: 0.0905, Val Loss: 0.0840\n",
            "Epoch [80/100], Train Loss: 0.0909, Val Loss: 0.0841\n",
            "Epoch [90/100], Train Loss: 0.0920, Val Loss: 0.0843\n",
            "Epoch [100/100], Train Loss: 0.0906, Val Loss: 0.0844\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_dim = X_train.shape[-1]\n",
        "d_model = 64\n",
        "nhead = 8\n",
        "num_layers = 3\n",
        "lstm_hidden_size = 256\n",
        "dropout = 0.1\n",
        "\n",
        "model = CustomModel(input_dim, d_model, nhead, num_layers, lstm_hidden_size, dropout).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# データローダーを作成\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(X_train, torch.tensor(y_train, dtype=torch.float32).to(device)), batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(TensorDataset(X_val, torch.tensor(y_val, dtype=torch.float32).to(device)), batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(X_test, torch.tensor(y_test, dtype=torch.float32).to(device)), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# モデルを訓練\n",
        "num_epochs = 100\n",
        "best_val_loss = float('inf')\n",
        "best_model_wts = model.state_dict()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_batch)\n",
        "        loss = criterion(output, y_batch.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * x_batch.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in val_loader:\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch.unsqueeze(1))\n",
        "            val_loss += loss.item() * x_batch.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_wts = model.state_dict()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "model.load_state_dict(best_model_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3kXh0ZXF5sZ",
        "outputId": "952b3cb9-7854-4db5-9653-7393dc21d6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.5203465223312378, 0.5144959688186646, 0.5079623460769653, 0.5045025944709778, 0.5016416311264038, 0.5001329779624939, 0.4978712201118469, 0.4983901381492615, 0.497533917427063, 0.5012999773025513, 0.5081307291984558, 0.5041447877883911, 0.4876466989517212, 0.48803505301475525, 0.4883384108543396, 0.4885469675064087, 0.4887157380580902, 0.48884105682373047, 0.48892539739608765, 0.48898839950561523, 0.489069402217865, 0.48911625146865845, 0.4892856478691101, 0.4895988702774048, 0.48991596698760986, 0.48988139629364014, 0.48986101150512695, 0.48984280228614807, 0.4898262023925781, 0.4898117184638977, 0.4897993803024292, 0.48978957533836365, 0.4897770583629608, 0.4897612929344177, 0.4897497296333313, 0.48973673582077026, 0.4897347092628479, 0.48973798751831055, 0.48974186182022095, 0.489744633436203, 0.4897457957267761, 0.4897478520870209, 0.4897497594356537, 0.48975247144699097, 0.4897555708885193, 0.4897586703300476, 0.48975953459739685, 0.4897586405277252, 0.48975837230682373, 0.48975828289985657, 0.48975902795791626, 0.4897589385509491, 0.48975878953933716, 0.48975837230682373, 0.4897578954696655, 0.4897571802139282, 0.48975667357444763, 0.4897557199001312, 0.48975586891174316, 0.48975589871406555, 0.4897559583187103, 0.4897559881210327, 0.4897560477256775, 0.48975619673728943, 0.4897562563419342, 0.4897565543651581, 0.48975709080696106, 0.4897577464580536, 0.48975861072540283, 0.48975950479507446, 0.4897593855857849, 0.4897591471672058, 0.48975878953933716, 0.48975878953933716, 0.48975831270217896, 0.4897575378417969, 0.48975664377212524, 0.4897557497024536, 0.48975586891174316, 0.48975592851638794, 0.4897559583187103, 0.4897559881210327, 0.4897560477256775, 0.48975619673728943, 0.4897562563419342, 0.48975658416748047, 0.48975706100463867, 0.4897577464580536, 0.48975861072540283, 0.4897593855857849, 0.4897593855857849, 0.4897591471672058, 0.48975878953933716, 0.48975878953933716, 0.48975831270217896, 0.4897575378417969, 0.48975664377212524, 0.4897557497024536, 0.48975586891174316, 0.48975592851638794]\n"
          ]
        }
      ],
      "source": [
        "# 訓練済みのモデルを使用して100日間のボラティリティ予測を生成\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "input_seq = X_test[:1].clone()  # 最初のテストサンプルを初期入力として選択し、元データを変更しないようにコピー\n",
        "for _ in range(100):\n",
        "    with torch.no_grad():\n",
        "        pred = model(input_seq)\n",
        "        predictions.append(pred.item())\n",
        "        # 入力シーケンスを更新し、最初の時間ステップを削除し、新しい予測値を追加\n",
        "        pred_expanded = pred.unsqueeze(-1).repeat(1, 1, input_seq.size(-1))\n",
        "        new_feature = torch.cat([input_seq[:, 1:, :], pred_expanded], dim=1)\n",
        "        input_seq = new_feature\n",
        "\n",
        "print(predictions)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
